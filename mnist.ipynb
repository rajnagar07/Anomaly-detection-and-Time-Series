{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPK98OvakPLQx0S1cKSs/mv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajnagar07/Anomaly-detection-and-Time-Series/blob/main/mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ymh_Mpy3WXH1"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import,division,print_function\n",
        "import tensorflow as tf\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MNIST dataset parameters\n",
        "num_classes = 10 #outpu (0:10) (0 to 9)\n",
        "num_feature = 784 #data features in our input(28*28 = 784)\n",
        "\n",
        "#traning paramter\n",
        "learning_rate = 0.001\n",
        "training_step = 3000\n",
        "batch_size = 256\n",
        "display_step = 100\n",
        "\n",
        "#Neural Network parameters\n",
        "n_hidden_1 = 128 #1st layer no of neurons\n",
        "n_hidden_2 = 256 #2nd layer no of neurons"
      ],
      "metadata": {
        "id": "d9bbdJp3Wphd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare MNIST data.\n",
        "\n",
        "# download the dataset\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Convert to float32\n",
        "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
        "\n",
        "# Flatten images to 1-D vector of 784 features (28* 28)\n",
        "x_train, x_test = x_train.reshape([-1, num_feature]), x_test.reshape([-1, num_feature])\n",
        "\n",
        "# Normalize image values from [0, 255] ---> [0, 1]\n",
        "x_train, x_test = x_train / 255. , x_test / 255."
      ],
      "metadata": {
        "id": "b0N4JAUVWpfD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66bf68ed-7ec3-478c-b94e-1514fd46d055"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use tf.data API to shuffle and batch data\n",
        "\n",
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"
      ],
      "metadata": {
        "id": "PjoIXEOyWpcD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#store layer weights and biases\n",
        "# A random number generator to intilize weights\n",
        "random_normal = tf.initializers.RandomNormal()\n",
        "weigths = {\n",
        "    \"h1\":tf.Variable(random_normal([num_feature,n_hidden_1])),\n",
        "    \"h2\":tf.Variable(random_normal([n_hidden_1,n_hidden_2])),\n",
        "    \"output\":tf.Variable(random_normal([n_hidden_2,num_classes]))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    \"b1\":tf.Variable(random_normal([n_hidden_1,])),\n",
        "    \"b2\":tf.Variable(random_normal([n_hidden_2])),\n",
        "    \"output\":tf.Variable(random_normal([num_classes]))\n",
        "}"
      ],
      "metadata": {
        "id": "EX8y9221WpZb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"shape W_1 : \",weigths[\"h1\"].shape)\n",
        "print(\"shape W_2 : \",weigths[\"h2\"].shape)\n",
        "print(\"shape W_out : \",weigths[\"output\"].shape)\n",
        "print(\"shape B1 : \",biases[\"b1\"].shape)\n",
        "print(\"shape B2 : \",biases[\"b2\"].shape)\n",
        "print(\"shape b_out : \",biases[\"output\"].shape)"
      ],
      "metadata": {
        "id": "_YzunQedWpW7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c21af06-2431-473f-bf0f-e1e3f6be3a7b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape W_1 :  (784, 128)\n",
            "shape W_2 :  (128, 256)\n",
            "shape W_out :  (256, 10)\n",
            "shape B1 :  (128,)\n",
            "shape B2 :  (256,)\n",
            "shape b_out :  (10,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#create a model\n",
        "def neural_net(x):\n",
        "  #hidden fully connected layer with 128 neuron\n",
        "  layer_1 = tf.add(tf.matmul(x,weigths['h1']),biases['b1'])  # z1\n",
        "  # apply ReLU to layer_1  output for non_linearity\n",
        "  layer_1 = tf.nn.relu(layer_1) # A1\n",
        "\n",
        "  #hidden fully connected layer with 256 neurons\n",
        "  layer_2 = tf.add(tf.matmul(layer_1,weigths['h2']),biases['b2']) #z2\n",
        "\n",
        "  #apply ReLU to layer_2 output\n",
        "  layer_2 = tf.nn.relu(layer_2) # A2\n",
        "\n",
        "  #output fully connected layer with a neuron for each class --> 10\n",
        "  out_layer = tf.add(tf.matmul(layer_2,weigths['output']),biases['output']) # z3\n",
        "\n",
        "  # apply  softmax to normalize to logits to a probablity distribution\n",
        "  return tf.nn.softmax(out_layer)"
      ],
      "metadata": {
        "id": "oinljCpvWpUM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #cross-entropy loss function\n",
        "# def cross_entropy(y_pred,y_true):\n",
        "#   #encode label to a one hot encode\n",
        "#   y_true = tf.one_hot(y_true,depth=num_classes)\n",
        "\n",
        "#   #clip prediction values to avoid log(0) error\n",
        "\n",
        "#   y_pred = tf.clip_by_value(y_pred,1e-9,1.)\n",
        "\n",
        "#   # compute cross-entropy loss\n",
        "#   return tf.reduce_mean(-tf.reduce_sum(y_true*tf.math.log(y_pred)))\n",
        "\n",
        "# Cross-Entropy loss function\n",
        "def cross_entropy(y_pred, y_true):\n",
        "  # Encode label to a one hot vector\n",
        "  y_true = tf.one_hot(y_true, depth = num_classes)\n",
        "\n",
        "  # Clip predictions values to avoid log(0) error.\n",
        "\n",
        "  y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n",
        "\n",
        "  # Compute cross-entropy loss\n",
        "\n",
        "  return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))\n",
        "\n",
        "#accuracy metrics\n",
        "def accuracy(y_pred,y_true):\n",
        "  #predicted class is the index of the highest score in prediction vector (ie agrmax)\n",
        "  correct_prediction = tf.equal(tf.argmax(y_pred,1),tf.cast(y_true,tf.int64))\n",
        "  return tf.reduce_mean(tf.cast(correct_prediction,tf.float32),axis=-1)\n",
        "\n",
        "#stohastic gradient descent optimizer\n",
        "optimizer = tf.optimizers.SGD(learning_rate)\n"
      ],
      "metadata": {
        "id": "Q9EKmlf6WpRs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#optimizing process\n",
        "\n",
        "def run_optimization(x,y):\n",
        "  #wrap computation insdie a gradient tape for automatic diffrentiation\n",
        "  with tf.GradientTape() as g:\n",
        "    pred = neural_net(x)\n",
        "    loss = cross_entropy(pred,y)\n",
        "\n",
        "    #variable to update ie trainable variable during back propagation\n",
        "    trainable_variables = list(weigths.values())+list(biases.values())\n",
        "\n",
        "    #compute gradients\n",
        "    gradients = g.gradient(loss,trainable_variables)\n",
        "\n",
        "    #update W and b following gradients\n",
        "    optimizer.apply_gradients(zip(gradients,trainable_variables))\n"
      ],
      "metadata": {
        "id": "NfbB-tznWpLz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run training for the given no of steps\n",
        "for step,(batch_x,batch_y) in enumerate(train_data.take(training_step),1):\n",
        "  # run the optimization top update W and b values after each batch\n",
        "  run_optimization(batch_x,batch_y)\n",
        "\n",
        "  if step % display_step == 0:\n",
        "    pred = neural_net(batch_x)\n",
        "    loss = cross_entropy(pred,batch_y)\n",
        "    acc = accuracy(pred,batch_y)\n",
        "\n",
        "    print(f\"step : {step},loss : {loss},accuracy : {acc}\")"
      ],
      "metadata": {
        "id": "z2ds3BHCWpJF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cd06d39-42cf-4d15-a160-3f1b3004713d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step : 100,loss : 85.8126220703125,accuracy : 0.9140625\n",
            "step : 200,loss : 37.466487884521484,accuracy : 0.96875\n",
            "step : 300,loss : 43.9863166809082,accuracy : 0.953125\n",
            "step : 400,loss : 29.249509811401367,accuracy : 0.97265625\n",
            "step : 500,loss : 29.703659057617188,accuracy : 0.96875\n",
            "step : 600,loss : 22.629322052001953,accuracy : 0.98046875\n",
            "step : 700,loss : 12.136271476745605,accuracy : 0.9921875\n",
            "step : 800,loss : 21.695171356201172,accuracy : 0.97265625\n",
            "step : 900,loss : 25.268321990966797,accuracy : 0.98046875\n",
            "step : 1000,loss : 18.890419006347656,accuracy : 0.9765625\n",
            "step : 1100,loss : 14.622906684875488,accuracy : 0.98046875\n",
            "step : 1200,loss : 7.242950439453125,accuracy : 1.0\n",
            "step : 1300,loss : 13.866117477416992,accuracy : 0.9921875\n",
            "step : 1400,loss : 6.49835205078125,accuracy : 1.0\n",
            "step : 1500,loss : 8.250457763671875,accuracy : 0.99609375\n",
            "step : 1600,loss : 8.844024658203125,accuracy : 0.99609375\n",
            "step : 1700,loss : 8.596818923950195,accuracy : 0.9921875\n",
            "step : 1800,loss : 8.100693702697754,accuracy : 0.99609375\n",
            "step : 1900,loss : 4.45377254486084,accuracy : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test model on validatation set\n",
        "pred = neural_net(x_test)\n",
        "print(f\"test acc : {accuracy(pred,y_test)}\")"
      ],
      "metadata": {
        "id": "GKrf7bl0WpGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visual prediction\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "metadata": {
        "id": "bWFr1epMWpDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predict 5 images\n",
        "n_images = 50\n",
        "# shape = [50,784]\n",
        "test_images = x_test[:n_images]\n",
        "\n",
        "#[50,10]\n",
        "preddictions = neural_net(test_images)\n",
        "\n",
        "#display images and model prediction\n",
        "for i in range(n_images):\n",
        "  plt.imshow(np.reshape(test_images[i],[28,28]),cmap='grey')\n",
        "  plt.show()\n",
        "  print(f\"model prediction : {np.argmax(preddictions.numpy()[i])}\")\n"
      ],
      "metadata": {
        "id": "cQmkKQVjWpA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CU496mUwWo99"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}